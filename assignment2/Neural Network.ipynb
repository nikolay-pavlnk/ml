{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W1\n",
      "Gradient check passed!\n",
      "Checking gradient for B1\n",
      "Gradient check passed!\n",
      "Checking gradient for W2\n",
      "Gradient check passed!\n",
      "Checking gradient for B2\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W1\n",
      "Gradient check passed!\n",
      "Checking gradient for B1\n",
      "Gradient check passed!\n",
      "Checking gradient for W2\n",
      "Gradient check passed!\n",
      "Checking gradient for B2\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки\n",
    "\n",
    "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.293639, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.331382, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.243131, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.323745, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.088912, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.156995, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.216808, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.292144, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.333013, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.315664, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.261385, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.122566, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.142476, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.194526, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.268271, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.117908, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.362601, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.323970, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.162284, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.263068, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-2)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f70609989e8>]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f706084cb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.959788, Train accuracy: 0.212667, val accuracy: 0.221000\n",
      "Loss: 2.810443, Train accuracy: 0.196778, val accuracy: 0.206000\n",
      "Loss: 2.363458, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.339646, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.290148, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.293556, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.340246, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.108798, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228876, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.308862, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.297575, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.160593, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.225978, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.304406, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.267677, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.114135, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.154985, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.100884, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.305363, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.223362, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 19.938765, Train accuracy: 0.161222, val accuracy: 0.166000\n",
      "Loss: 16.958464, Train accuracy: 0.197444, val accuracy: 0.197000\n",
      "Loss: 14.492287, Train accuracy: 0.209889, val accuracy: 0.218000\n",
      "Loss: 12.618399, Train accuracy: 0.213444, val accuracy: 0.226000\n",
      "Loss: 10.977324, Train accuracy: 0.214556, val accuracy: 0.222000\n",
      "Loss: 9.715602, Train accuracy: 0.212333, val accuracy: 0.219000\n",
      "Loss: 8.437718, Train accuracy: 0.208667, val accuracy: 0.214000\n",
      "Loss: 7.463225, Train accuracy: 0.206667, val accuracy: 0.213000\n",
      "Loss: 6.748350, Train accuracy: 0.205333, val accuracy: 0.213000\n",
      "Loss: 5.918101, Train accuracy: 0.203778, val accuracy: 0.211000\n",
      "Loss: 5.552009, Train accuracy: 0.202111, val accuracy: 0.210000\n",
      "Loss: 4.985722, Train accuracy: 0.200111, val accuracy: 0.207000\n",
      "Loss: 4.628989, Train accuracy: 0.199333, val accuracy: 0.207000\n",
      "Loss: 4.238030, Train accuracy: 0.197889, val accuracy: 0.206000\n",
      "Loss: 3.998221, Train accuracy: 0.197222, val accuracy: 0.206000\n",
      "Loss: 3.717195, Train accuracy: 0.197111, val accuracy: 0.206000\n",
      "Loss: 3.444410, Train accuracy: 0.197000, val accuracy: 0.206000\n",
      "Loss: 3.381462, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 3.283405, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.970597, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 21.374876, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 19.051324, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 16.651458, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 14.594582, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 12.864795, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 12.388472, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 10.842245, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 9.586896, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 8.752924, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 7.633779, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 6.828739, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 6.369760, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 5.708310, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 5.325574, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 4.679985, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 4.479309, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 4.089431, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 3.441857, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 3.431180, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 3.260503, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 3.050232, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.890424, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.728682, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.298923, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.169093, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.267653, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.251601, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.003948, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.917564, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.044669, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.767506, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.681627, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.591412, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.775379, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.791155, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.766059, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.598887, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.460655, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.450208, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.682266, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.349724, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.146742, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.305890, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.335910, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.520856, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.308548, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.469069, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.284302, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.432474, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.259661, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.203229, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.287923, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.115633, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.142061, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.438234, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.244308, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.150502, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.169358, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.271722, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.195966, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.407214, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.179999, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.313140, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.360720, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.240966, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.217474, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.175062, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.313516, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.237527, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.276566, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.280212, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.065696, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.297631, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.178964, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.159501, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.263178, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.118477, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.170110, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.174324, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.076659, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.117908, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.082999, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.185787, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.086967, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.214905, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.215769, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.043167, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.257264, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.246800, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.145925, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.147084, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.235328, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.280576, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.293484, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.241144, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.238588, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.277277, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.189096, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.158719, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.275550, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.101146, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.386811, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.058246, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.400303, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.256934, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.157033, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.120682, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.127741, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.242728, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.181878, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.198520, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.171869, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.132241, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.064577, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.150250, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.070922, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.184622, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.231262, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.023551, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.024629, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.366098, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.175095, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.108002, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.089509, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.061342, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.053780, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.139201, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.240571, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.009454, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.176335, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.289641, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.226521, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.310165, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.207816, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.109094, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.161196, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.177317, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.292173, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.076073, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.238660, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.124724, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.085249, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.208011, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.206139, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.208842, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.169755, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.173944, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.326019, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.112550, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.236060, Train accuracy: 1.000000, val accuracy: 0.066667\n"
     ]
    }
   ],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.659986, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 3.610599, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 3.661780, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.918763, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.627560, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.400381, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.331302, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.336832, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.101700, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.240602, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.198261, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.117779, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.991601, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.072640, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.052095, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.911227, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.903598, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.881326, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.872710, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.879317, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-2)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **60%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.365096, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.160626, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.058380, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.374260, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.326371, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.319729, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.489240, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.307766, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.459314, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.183502, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 1.951422, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.097149, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.460607, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.189889, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 1.967995, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.295400, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 1.952473, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.217743, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 1.914588, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.142995, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.009563, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.306234, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.242325, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.398925, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.333435, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.295581, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.277843, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.252647, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.294132, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.125649, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.075582, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.326869, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.366151, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.291867, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.003237, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.220381, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.359696, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 1.880478, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.221735, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.032001, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.252174, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.063649, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.165009, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.114185, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.289818, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.089005, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.411559, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.423720, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.115269, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.235662, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 1.840600, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.223064, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.069679, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.281856, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.105160, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.208978, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.271962, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.236506, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.416232, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.353095, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.455532, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.127250, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.332214, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.160802, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.494420, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.330434, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.094688, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.239211, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.105515, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.071555, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.268331, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.372373, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.343268, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.313086, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.251810, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.350933, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.266638, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.534893, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.442353, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.303799, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.085810, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.161662, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.324090, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.172207, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.126443, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.118266, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.467985, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.351470, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.172606, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.187415, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 1.902194, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.235267, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.209091, Train accuracy: 0.225889, val accuracy: 0.233000\n",
      "Loss: 2.125545, Train accuracy: 0.225889, val accuracy: 0.233000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-8e75d6fa5dd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mbest_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/DL/dlcourse_ai-master/assignments/assignment2/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 loss = self.model.compute_loss_and_gradients(self.dataset.train_X[batch_indices],\n\u001b[0;32m--> 101\u001b[0;31m                     self.dataset.train_y[batch_indices])\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/DL/dlcourse_ai-master/assignments/assignment2/model.py\u001b[0m in \u001b[0;36mcompute_loss_and_gradients\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mgrad_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msecond_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mgrad_activation_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_activation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mgrad_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_activation_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/DL/dlcourse_ai-master/assignments/assignment2/layers.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, d_out)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \"\"\"\n\u001b[1;32m    155\u001b[0m         \u001b[0md_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mgrad_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0mgrad_biases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgrad_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rates = [0.001]\n",
    "reg_strength = [0.001]\n",
    "learning_rate_decay = [0.98999999999999999]\n",
    "hidden_layer_size = [512]\n",
    "num_epochs = [300]\n",
    "batch_size = [32]\n",
    "\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "for lr, reg, dec, size, epochs, batch in list(product(*(learning_rates, reg_strength, learning_rate_decay, hidden_layer_size, num_epochs, batch_size))):\n",
    "    model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = size, reg = reg)\n",
    "    trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=lr, num_epochs=epochs, batch_size=batch, learning_rate_decay=dec)\n",
    "    loss_history, train_history, val_history = trainer.fit()\n",
    "    accuracy = trainer.compute_accuracy(val_X, val_y)\n",
    "    \n",
    "    if accuracy > best_val_accuracy:\n",
    "        best_classifier = model\n",
    "        best_val_accuracy = accuracy\n",
    "        best_params = {'reg': reg, 'lr': lr, 'decay': dec, 'hidden': size, 'epochs': epochs, 'batch': batch}\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch': 32,\n",
       " 'decay': 0.98999999999999999,\n",
       " 'epochs': 300,\n",
       " 'hidden': 254,\n",
       " 'lr': 0.001,\n",
       " 'reg': 0.001}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0e651550b8>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3QAAAGrCAYAAACIZ9VoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkiRpjfr/25goFlTeDw8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0e65296f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural net test set accuracy: 0.742000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 3, 0, 2], dtype=uint8)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 3, 9, 2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
